{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45098534-76e6-4aa1-9131-b0cf1b5458af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ff2cdb-7323-44f5-bec4-eafe098f0c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install googlesearch-python lxml deepspeed sentence-transformers \n",
    "# !pip install feedparser sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cb884c2-8e0d-459c-a2d2-07d56734e661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "#  \n",
    "# Cell 2: Imports\n",
    "#\n",
    "# Description: All required libraries for the application are imported here.\n",
    "#\n",
    "\n",
    "import os\n",
    "import json\n",
    "import socket\n",
    "from typing import List, Dict\n",
    "import subprocess\n",
    "import time\n",
    "import re\n",
    "import gradio as gr\n",
    "\n",
    "# LangChain and related libraries\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Search library\n",
    "try:\n",
    "    from googlesearch import search\n",
    "except ImportError:\n",
    "    print(\"Error: 'googlesearch-python' is not installed. Please run 'pip install googlesearch-python'\")\n",
    "\n",
    "# Set a user agent to avoid being blocked by Google search\n",
    "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4351c5cb-e018-44cf-a752-b6fc9919a648",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Agent 1 - Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cdcd502-f05b-4322-b9c7-6d13ed1c802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Cell 3: Agent 1 - User Profiler\n",
    "#\n",
    "# Description: This agent is responsible for understanding the user's knowledge level.\n",
    "# It dynamically generates a questionnaire, analyzes the answers, and creates a\n",
    "# profile that will be used by Agent 3 to tailor its responses.\n",
    "#\n",
    "\n",
    "class Questionnaire(BaseModel):\n",
    "    questions: List[str] = Field(description=\"A list of 4-5 questions for the user.\")\n",
    "\n",
    "class UserProfilerAgent_V3:\n",
    "    \"\"\"\n",
    "    Agent 1 (V3): Guarantees the user's name is collected first before\n",
    "    using an LLM to dynamically generate the rest of the questionnaire.\n",
    "    **ADAPTED FOR MODULAR, GRADIO-FRIENDLY USE.**\n",
    "    \"\"\"\n",
    "    def __init__(self, profiles_dir: str = \"user_profiles\"):\n",
    "        self.profiles_dir = profiles_dir\n",
    "        if not os.path.exists(self.profiles_dir):\n",
    "            os.makedirs(self.profiles_dir)\n",
    "\n",
    "        try:\n",
    "            host_node = socket.gethostname()\n",
    "            # NOTE: The ASURITE ID should be that of the user running the Ollama server.\n",
    "            # This is specified as a hackathon resource.\n",
    "            asurite_id = \"apoojar4\"\n",
    "            self.llm = ChatOllama(model=\"qwen3:14b\", base_url=f\"http://{asurite_id}@{host_node}:11434/\")\n",
    "            self.structured_llm = self.llm.with_structured_output(Questionnaire)\n",
    "            print(\"✅ [Agent 1] Successfully connected to Ollama LLM.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ [Agent 1] Error connecting to Ollama: {e}\")\n",
    "            self.llm = None\n",
    "\n",
    "    def get_user_list(self) -> List[str]:\n",
    "        \"\"\"Scans the profiles directory and returns a list of user names.\"\"\"\n",
    "        if not os.path.exists(self.profiles_dir):\n",
    "            return []\n",
    "        files = [f for f in os.listdir(self.profiles_dir) if f.endswith('.txt')]\n",
    "        # Convert 'first_last.txt' to 'First Last'\n",
    "        names = [\" \".join(f.replace('.txt', '').split('_')).title() for f in files]\n",
    "        return names\n",
    "\n",
    "    def _generate_questions_with_llm(self) -> List[str]:\n",
    "        \"\"\"Uses an LLM to dynamically generate a user questionnaire.\"\"\"\n",
    "        if not self.llm: # Fallback if LLM is not available\n",
    "            return [\n",
    "                \"On a scale of 1-5, how comfortable are you with Python?\",\n",
    "                \"Which Python data science libraries (like Pandas or NumPy) have you used before?\",\n",
    "                \"Have you ever heard of using GPUs to speed up data analysis?\",\n",
    "                \"What's the first tool you'd reach for to do a large matrix multiplication in Python?\"\n",
    "            ]\n",
    "            \n",
    "        print(\"\\n🤖 [Agent 1] Generating a personalized questionnaire...\")\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"\"\"\n",
    "            You are a helpful assistant for an AI Data Science Tutor. Your goal is to create a short questionnaire (4-5 questions) to understand a user's knowledge level.\n",
    "            The questions should gently probe their experience with:\n",
    "            1. The Python programming language.\n",
    "            2. Common CPU-based data science libraries (like NumPy, Pandas).\n",
    "            3. Their awareness of GPU computing and hardware acceleration.\n",
    "            4. Their familiarity with any NVIDIA-specific GPU libraries (like CuPy or RAPIDS).\n",
    "            IMPORTANT: Do NOT ask for the user's name, as it will be collected separately.\n",
    "            Return the questions as a JSON list. Be conversational and friendly.\n",
    "            \"\"\",\n",
    "            input_variables=[],\n",
    "        )\n",
    "        query_generation_chain = prompt | self.structured_llm\n",
    "        try:\n",
    "            response_model = query_generation_chain.invoke({})\n",
    "            return response_model.questions\n",
    "        except Exception as e:\n",
    "            print(f\"-> [Agent 1] LLM failed to generate questions, falling back to default. Error: {e}\")\n",
    "            return [\n",
    "                \"On a scale of 1-5, how comfortable are you with Python?\",\n",
    "                \"Which Python data science libraries (like Pandas or NumPy) have you used before?\",\n",
    "                \"Have you ever heard of using GPUs to speed up data analysis?\",\n",
    "                \"What's the first tool you'd reach for to do a large matrix multiplication in Python?\"\n",
    "            ]\n",
    "\n",
    "    def generate_and_save_report(self, user_name: str, answers_dict: dict) -> str:\n",
    "        \"\"\"\n",
    "        Takes a user name and a dictionary of answers, generates a report with an LLM,\n",
    "        and saves it to a file. Returns the path to the saved file.\n",
    "        \"\"\"\n",
    "        if not self.llm:\n",
    "            return \"Error: LLM not connected.\"\n",
    "\n",
    "        print(f\"\\n🤖 [Agent 1] Analyzing responses for {user_name} and creating a profile...\")\n",
    "        answers_str = \"\\n\".join([f\"- {q}: {a}\" for q, a in answers_dict.items()])\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"\"\"\n",
    "            You are an expert AI analyst. A user named {user_name} has answered a questionnaire about their data science skills.\n",
    "            Your task is to analyze their answers and generate a \"TUTORING STRATEGY\" report for our AI Tutor.\n",
    "            **User's Answers:**\n",
    "            {answers}\n",
    "            **Your Task:**\n",
    "            1.  Determine the user's knowledge level: 'Beginner', 'Intermediate', or 'Advanced'.\n",
    "            2.  Write a concise report following the correct strategy format below. This report will be given to another AI, so the instructions must be clear.\n",
    "            ---\n",
    "            **STRATEGY FORMATS (Choose ONE):**\n",
    "            **If 'Beginner':**\n",
    "            Start with `Knowledge Level: Beginner`. On the next line, start with `TUTORING STRATEGY: The user is a beginner.` Then, explain that the tutor should use high-level concepts, explain the 'why' of GPU acceleration, and introduce NVIDIA libraries (like CuPy) as a simple, powerful alternative.\n",
    "            **If 'Intermediate':**\n",
    "            Start with `Knowledge Level: Intermediate`. On the next line, start with `TUTORING STRATEGY: The user is at an intermediate level.` Then, explain that the tutor should provide direct code comparisons (e.g., NumPy vs. CuPy), focus on performance benefits, and show clear benchmarking examples.\n",
    "            **If 'Advanced':**\n",
    "            Start with `Knowledge Level: Advanced`. On the next line, start with `TUTORING STRATEGY: The user is advanced.` Then, explain that the tutor can provide nuanced advice, discuss the broader NVIDIA RAPIDS ecosystem, and cover specific benchmarking methodologies on the Sol supercomputer.\n",
    "            ---\n",
    "            Now, generate the complete report.\n",
    "            \"\"\",\n",
    "            input_variables=[\"user_name\", \"answers\"],\n",
    "        )\n",
    "        report_generation_chain = prompt | self.llm\n",
    "        response_message = report_generation_chain.invoke({\"user_name\": user_name, \"answers\": answers_str})\n",
    "        report_content = response_message.content\n",
    "\n",
    "        knowledge_level = \"Beginner\"\n",
    "        match = re.search(r\"Knowledge Level:\\s*(\\w+)\", report_content, re.IGNORECASE)\n",
    "        if match:\n",
    "            knowledge_level = match.group(1)\n",
    "            \n",
    "        full_report = f\"--- User Profile for {user_name} ---\\n{report_content}\\n--- End of Profile ---\"\n",
    "\n",
    "        # Save the report to a text file\n",
    "        filename = \"_\".join(user_name.lower().split()) + \".txt\"\n",
    "        filepath = os.path.join(self.profiles_dir, filename)\n",
    "        with open(filepath, \"w\") as f:\n",
    "            f.write(full_report)\n",
    "        print(f\"\\n✅ [Agent 1] User profile report saved successfully to: {filepath}\")\n",
    "        return filepath, knowledge_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eadc4b8-19c6-401e-ab23-ff3977c29f9b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Agent 4 - Benchmarker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d122e961-c4af-4a01-9299-0e419a6b1412",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolBenchmarker:\n",
    "    def __init__(self, user: str, python_env: str = \"rapids25.02\"):\n",
    "        if not user or user == \"YOUR_ASURITE_ID\":\n",
    "            raise ValueError(\"A valid ASURITE username is required for SolBenchmarker.\")\n",
    "        self.user = user\n",
    "        self.python_env = python_env\n",
    "\n",
    "    def _generate_sbatch_script(self, script_dir: str, script_name: str, job_type: str) -> str:\n",
    "        # This function now generates a specific script for either a CPU or GPU job\n",
    "        \n",
    "        # if job_type == \"gpu\":\n",
    "        #     # The GPU job requests a GPU resource\n",
    "        #     resources = \"#SBATCH -G 1\"\n",
    "        # else: # cpu\n",
    "        #     # The CPU job requests a standard node without a GPU\n",
    "        #     resources = \"#SBATCH --nodes=1\" \n",
    "\n",
    "        return f\"\"\"#!/bin/bash\n",
    "#SBATCH -p general\n",
    "#SBATCH -q public\n",
    "#SBATCH -G 1\n",
    "#SBATCH -A grp_hackathon2025\n",
    "#SBATCH --reservation=hackathon2025\n",
    "#SBATCH -t 0-00:10:00\n",
    "#SBATCH -c 1\n",
    "#SBATCH -o {script_dir}/slurm-{job_type}-%j.out\n",
    "#SBATCH -e {script_dir}/slurm-{job_type}-%j.err\n",
    "\n",
    "module load mamba/latest\n",
    "source activate {self.python_env}\n",
    "\n",
    "echo \"--- STARTING {job_type.upper()} BENCHMARK ---\"\n",
    "/usr/bin/time -p python3 {script_dir}/{script_name} 2>&1\n",
    "echo \"--- FINISHED {job_type.upper()} BENCHMARK ---\"\n",
    "\"\"\"\n",
    "\n",
    "    def _submit_job(self, code: str, job_type: str, benchmark_dir: str) -> str:\n",
    "        # Helper to submit a single job and return its ID\n",
    "        script_name = f\"{job_type}_benchmark.py\"\n",
    "        script_path = os.path.join(benchmark_dir, script_name)\n",
    "        sbatch_path = os.path.join(benchmark_dir, f\"{job_type}_job.sh\")\n",
    "        \n",
    "        with open(script_path, \"w\") as f: f.write(code)\n",
    "        sbatch_content = self._generate_sbatch_script(benchmark_dir, script_name, job_type)\n",
    "        with open(sbatch_path, \"w\") as f: f.write(sbatch_content)\n",
    "\n",
    "        process = subprocess.run(f\"sbatch {sbatch_path}\", shell=True, capture_output=True, text=True)\n",
    "        if process.returncode != 0: raise RuntimeError(f\"sbatch submission for {job_type} failed: {process.stderr}\")\n",
    "        \n",
    "        job_id_match = re.search(r\"Submitted batch job (\\d+)\", process.stdout.strip())\n",
    "        if not job_id_match: raise RuntimeError(f\"Could not parse Job ID for {job_type}: {process.stdout}\")\n",
    "        \n",
    "        job_id = job_id_match.group(1)\n",
    "        print(f\"--> [Agent 4] Submitted {job_type.upper()} job with ID: {job_id}\")\n",
    "        return job_id\n",
    "\n",
    "\n",
    "    def _check_job_completion(self, job_id: str, job_type: str, benchmark_dir: str) -> dict:\n",
    "        \"\"\"\n",
    "        Helper to check if a job is done using the robust 'sacct' command.\n",
    "        This avoids the race condition present with 'squeue'.\n",
    "        \"\"\"\n",
    "        # Command to get the state of the specific job, with no header.\n",
    "        command = f\"sacct -j {job_id} -o State --noheader\"\n",
    "        process = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "        \n",
    "        # The output can have multiple lines (e.g., for each step), so we check the first.\n",
    "        status_lines = [line.strip() for line in process.stdout.splitlines() if line.strip()]\n",
    "        \n",
    "        if not status_lines:\n",
    "            # If sacct returns nothing, the job might still be initializing. Treat as running.\n",
    "            return {\"status\": \"running\"}\n",
    "\n",
    "        # Get the primary status, removing \"(See...\" if present\n",
    "        primary_status = status_lines[0].split()[0]\n",
    "        \n",
    "        if primary_status in [\"PENDING\", \"RUNNING\", \"CONFIGURING\"]:\n",
    "            return {\"status\": \"running\"}\n",
    "        \n",
    "        # If the status is anything else, the job is considered finished.\n",
    "        print(f\"--> [Agent 4] {job_type.upper()} Job {job_id} finished with status: {primary_status}\")\n",
    "        \n",
    "        if primary_status == \"COMPLETED\":\n",
    "            # Add a small delay to ensure the output file has been fully written by the filesystem\n",
    "            time.sleep(2)\n",
    "            output_file_path = os.path.join(benchmark_dir, f\"slurm-{job_type}-{job_id}.out\")\n",
    "            if os.path.exists(output_file_path):\n",
    "                with open(output_file_path, \"r\") as f:\n",
    "                    output_content = f.read()\n",
    "                real_time_match = re.search(r\"real\\s+([\\d.]+)\", output_content)\n",
    "                time_val = float(real_time_match.group(1)) if real_time_match else None\n",
    "                return {\"status\": \"complete\", \"job_type\": job_type, \"time\": time_val}\n",
    "            else:\n",
    "                return {\"status\": \"error\", \"job_type\": job_type, \"message\": \"Job COMPLETED but output file not found.\"}\n",
    "        else: # FAILED, CANCELLED, TIMEOUT, etc.\n",
    "            return {\"status\": \"error\", \"job_type\": job_type, \"message\": f\"Job finished with non-COMPLETED status: {primary_status}\"}\n",
    "\n",
    "#         --> [Agent 4] GPU Job 28620141 completed.\n",
    "# --> [Agent 4] CPU Job 28620142 completed.\n",
    "\n",
    "    def run_benchmark_parallel(self, cpu_code: str, gpu_code: str):\n",
    "        # The main method, now a generator that yields results as they complete.\n",
    "        benchmark_dir = os.path.join(os.getcwd(), \"benchmark_files\")\n",
    "        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            gpu_job_id = self._submit_job(gpu_code, \"gpu\", benchmark_dir)\n",
    "            cpu_job_id = self._submit_job(cpu_code, \"cpu\", benchmark_dir)\n",
    "\n",
    "            active_jobs = {\"gpu\": gpu_job_id, \"cpu\": cpu_job_id}\n",
    "\n",
    "            while active_jobs:\n",
    "                # Check the status of each active job\n",
    "                for job_type, job_id in list(active_jobs.items()):\n",
    "                    result = self._check_job_completion(job_id, job_type, benchmark_dir)\n",
    "                    if result[\"status\"] != \"running\":\n",
    "                        yield result # Yield the result as soon as a job finishes\n",
    "                        del active_jobs[job_type] # Remove from active monitoring\n",
    "                \n",
    "                if active_jobs:\n",
    "                    time.sleep(5) # Wait before polling again\n",
    "\n",
    "        except Exception as e:\n",
    "            yield {\"status\": \"error\", \"message\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dd130d8-4e78-48ce-9ce5-6bd897dbd2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NEW: Code Upscaler Function with Strict Constraints ---\n",
    "\n",
    "# NOT USING AS SUCH IN THE PIPELINE\n",
    "def _upscale_code_agentic(original_code: str, llm, library_string: str) -> str:\n",
    "    print(\"--> [Agent 3] Invoking LLM with STRICT CONSTRAINTS to upscale code...\")\n",
    "    \n",
    "    # This class is used to ensure the LLM returns only a code string.\n",
    "    class CodeOutput(BaseModel):\n",
    "        code: str = Field(description=\"The complete, upscaled Python code.\")\n",
    "        \n",
    "    # THE NEW, MORE SPECIFIC PROMPT\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are a benchmark preparation assistant. Your task is to analyze the following Python script and make it suitable for a meaningful performance benchmark.\n",
    "\n",
    "        **Your instructions are very strict:**\n",
    "        1.  **DO NOT** change any of the `import` statements.\n",
    "        2.  **ONLY** find the variable that defines the primary data size (e.g., `size = 10`, `n_rows = 1000`).\n",
    "        3.  Change the value of that single variable to a much larger number to ensure the benchmark is meaningful. For example, change matrix dimensions to at least 8000 or DataFrame rows to over 5 million.\n",
    "        4.  Return the complete, modified Python script and nothing else.\n",
    "\n",
    "        **Constraint:** The final code must only use libraries from this list: {available_libraries}\n",
    "\n",
    "        Original Code:\n",
    "        ```python\n",
    "        {original_code}\n",
    "        ```\n",
    "\n",
    "        Upscaled Code:\n",
    "        \"\"\",\n",
    "        input_variables=[\"original_code\", \"available_libraries\"],\n",
    "    )\n",
    "    \n",
    "    # The chain now uses the new, stricter prompt\n",
    "    structured_chain = prompt_template | llm.with_structured_output(CodeOutput)\n",
    "    \n",
    "    try:\n",
    "        response_model = structured_chain.invoke({\n",
    "            \"original_code\": original_code,\n",
    "            \"available_libraries\": library_string\n",
    "        })\n",
    "        upscaled_code = response_model.code\n",
    "        print(\"--> [Agent 3] Code successfully upscaled.\")\n",
    "        return upscaled_code\n",
    "    except Exception as e:\n",
    "        print(f\"--> [Agent 3] Could not upscale code, using original. Error: {e}\")\n",
    "        # Fallback to original code if upscaling fails\n",
    "        return original_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d282d80-7f9b-4ed0-9714-2430235e8602",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Agent 2 - Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e908618-cce9-4711-b02f-daba7f1fdbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# AGENTS 2 & 3 HELPER FUNCTIONS\n",
    "# This cell contains all the supporting logic for retrieval and code processing.\n",
    "# =================================================================\n",
    "from sentence_transformers import util\n",
    "import feedparser\n",
    "\n",
    "class SearchQueryGenerator(BaseModel):\n",
    "    queries: List[str] = Field(description=\"A list of targeted, keyword-focused search queries.\")\n",
    "\n",
    "def generate_search_queries(query: str, llm) -> List[str]:\n",
    "    print(\"-> [Agent 2] Using LLM to generate search queries...\")\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are an expert at generating web search queries. Analyze the user's question to identify the core technical task and programming language.\n",
    "        Generate 3 concise, targeted search queries. One query should be for the standard, CPU-based approach. Two queries should be for GPU-accelerated approaches, prioritizing NVIDIA-based solutions if they exist.\n",
    "        User Question: \"{question}\"\n",
    "        Generate a JSON list of 3 search query strings.\n",
    "        \"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )\n",
    "    query_generation_chain = prompt_template | llm.with_structured_output(SearchQueryGenerator)\n",
    "    try:\n",
    "        response_model = query_generation_chain.invoke({\"question\": query})\n",
    "        print(f\"-> [Agent 2] Generated queries: {response_model.queries}\")\n",
    "        return response_model.queries\n",
    "    except Exception as e:\n",
    "        print(f\"-> [Agent 2] LLM failed to generate structured output: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- NEW: Hybrid Retriever Functions (Tier 1, 2, 3) ---\n",
    "\n",
    "def fetch_articles_from_rss(query: str, embedding_model) -> list[str]:\n",
    "    print(\"-> [Tier 1] Trying Semantic Search on RSS feeds...\")\n",
    "    RSS_FEEDS = {\n",
    "        \"NVIDIA Developer Blog\": \"https://developer.nvidia.com/blog/feed/\",\n",
    "        \"RAPIDS AI (Medium)\": \"https://medium.com/feed/rapids-ai\",\n",
    "        \"CuPy (Medium)\": \"https://medium.com/feed/cupy-team\"\n",
    "    }\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    found_articles = []\n",
    "    for name, url in RSS_FEEDS.items():\n",
    "        feed = feedparser.parse(url)\n",
    "        for entry in feed.entries:\n",
    "            entry_text = entry.title + \". \" + entry.summary\n",
    "            entry_embedding = embedding_model.embed_query(entry_text)\n",
    "            similarity = util.pytorch_cos_sim(query_embedding, entry_embedding)\n",
    "            if similarity.item() > 0.35: # Keep a minimal threshold\n",
    "                found_articles.append({'link': entry.link, 'similarity': similarity.item()})\n",
    "    \n",
    "    found_articles.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    final_urls = [article['link'] for article in found_articles[:5]]\n",
    "    print(f\"-> [Tier 1] Found {len(final_urls)} semantically relevant URLs from RSS.\")\n",
    "    return final_urls\n",
    "\n",
    "def dynamic_web_search(queries: List[str]) -> list[str]:\n",
    "    print(\"-> [Tier 2] Falling back to polite Web Search...\")\n",
    "    all_urls = set()\n",
    "    for q in queries:\n",
    "        try:\n",
    "            search_results = list(search(q, num_results=2))\n",
    "            all_urls.update(url for url in search_results if url)\n",
    "            time.sleep(2)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during web search: {e}\")\n",
    "            continue\n",
    "    return list(all_urls)\n",
    "\n",
    "def hybrid_retriever(query: str, llm, embedding_model) -> list[str]:\n",
    "    # 1. First, try the high-quality RSS feeds\n",
    "    urls = fetch_articles_from_rss(query, embedding_model)\n",
    "\n",
    "    # 2. If RSS finds nothing, fallback to a general web search\n",
    "    if not urls:\n",
    "        print(\"--> RSS search found no relevant articles. Falling back to web search.\")\n",
    "        search_queries = generate_search_queries(query, llm)\n",
    "        if search_queries:\n",
    "            urls = dynamic_web_search(search_queries)\n",
    "\n",
    "    # 3. If both searches fail, use a final safety net of default links\n",
    "    if not urls:\n",
    "        print(\"--> All searches failed. Using default fallback links.\")\n",
    "        urls = [\n",
    "            \"https://developer.nvidia.com/blog/icymi-leveraging-the-power-of-gpus-with-cupy-in-python/\",\n",
    "            \"https://medium.com/rapids-ai/rapids-23-08-release-23db51c255f0\"\n",
    "        ]\n",
    "        \n",
    "    print(f\"-> [Hybrid Retriever] Found {len(urls)} final URLs to use for context.\")\n",
    "    return urls\n",
    "\n",
    "# --- Other Helper Functions ---\n",
    "def _get_available_libraries(filepath: str) -> List[str]:\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"--> Library file not found at {filepath}. Using a default list.\")\n",
    "        return ['numpy', 'cupy', 'pandas', 'cudf', 'cuml', 'scipy', 'sklearn']\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    libraries = []\n",
    "    for line in lines:\n",
    "        if not line.startswith('#'):\n",
    "            package_name = line.split()[0]\n",
    "            libraries.append(package_name)\n",
    "    print(f\"--> Successfully loaded {len(libraries)} available libraries from file.\")\n",
    "    return libraries\n",
    "\n",
    "# DEFINITIVE, \"PROCESS OF ELIMINATION\" CODE EXTRACTOR\n",
    "def _extract_python_code(markdown_text: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    The definitive helper function to parse CPU and GPU code blocks. It uses\n",
    "    a \"process of elimination\" to robustly identify the CPU and GPU code.\n",
    "    \"\"\"\n",
    "    print(\"--> [Agent 3] Extracting code blocks using definitive 'Process of Elimination' parser...\")\n",
    "    \n",
    "    cpu_code = \"\"\n",
    "    gpu_code = \"\"\n",
    "    \n",
    "    # This pattern finds all sections that have a \"Solution\" heading and a python code block\n",
    "    pattern = re.compile(\n",
    "        r\"(###?\\s*.*?Solution.*?)\\n*```python\\n(.*?)\\n```\", \n",
    "        re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    matches = pattern.findall(markdown_text)\n",
    "    \n",
    "    # This list will hold any code that isn't explicitly a GPU solution\n",
    "    other_code_blocks = []\n",
    "    \n",
    "    for heading, code in matches:\n",
    "        # First, positively identify the GPU block\n",
    "        if \"gpu\" in heading.lower():\n",
    "            gpu_code = code.strip()\n",
    "        else:\n",
    "            # If it's not a GPU block, add it to a temporary list\n",
    "            other_code_blocks.append(code.strip())\n",
    "\n",
    "    # If we found a GPU block AND there's another block left over,\n",
    "    # that other block must be the CPU solution.\n",
    "    if gpu_code and other_code_blocks:\n",
    "        cpu_code = other_code_blocks[0]\n",
    "    # If we only found one block and it wasn't the GPU one, it must be the CPU one.\n",
    "    elif not gpu_code and other_code_blocks:\n",
    "        cpu_code = other_code_blocks[0]\n",
    "            \n",
    "    # Your helpful debugging prints\n",
    "    print(\"################################################\")\n",
    "    print(\"GPU Code Found:\\n\", gpu_code)\n",
    "    print(\"################################################\")\n",
    "    print(\"CPU Code Found:\\n\", cpu_code)\n",
    "    print(\"################################################\")\n",
    "\n",
    "    return {\"cpu_code\": cpu_code, \"gpu_code\": gpu_code}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8345a9-6913-4473-b7ec-e9bf3cf0320e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Agent N - Router Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a9221fe-6a41-41ed-a530-6eef03c49553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Cell 8, REPLACE the RouterAgent class\n",
    "\n",
    "class RouterAgent:\n",
    "    \"\"\"A more precise agent to classify user intent and choose an execution path.\"\"\"\n",
    "    \n",
    "    class Path(BaseModel):\n",
    "        path_name: str = Field(description=\"The chosen execution path. Must be one of: 'CONVERSATIONAL', 'FOLLOW_UP', or 'COMPLEX_QUERY'.\")\n",
    "\n",
    "    def __init__(self, llm):\n",
    "        self.prompt = PromptTemplate(\n",
    "            template=\"\"\"\n",
    "            You are a routing agent. Your job is to classify the user's latest message by following these rules in order:\n",
    "\n",
    "            1.  First, check for simple \"chit-chat\". If the message is a greeting (hi, hello), a thank you, a goodbye, or asks about you (\"who are you?\", \"what can you do?\"), choose the path 'CONVERSATIONAL'.\n",
    "\n",
    "            2.  If it's not chit-chat, analyze the CONVERSATION HISTORY. If the latest message is short (e.g., \"why?\", \"can you explain that differently?\") AND it directly refers to the AI's immediately preceding answer, choose the path 'FOLLOW_UP'.\n",
    "\n",
    "            3.  If it's none of the above, it is a new technical question. Choose the path 'COMPLEX_QUERY'.\n",
    "\n",
    "            CONVERSATION HISTORY:\n",
    "            {chat_history}\n",
    "\n",
    "            USER'S LATEST MESSAGE:\n",
    "            {question}\n",
    "\n",
    "            Chosen Path:\n",
    "            \"\"\",\n",
    "            input_variables=[\"chat_history\", \"question\"],\n",
    "        )\n",
    "        self.chain = self.prompt | llm.with_structured_output(self.Path)\n",
    "\n",
    "    def choose_path(self, query: str, chat_history: List[Dict]) -> str:\n",
    "        # This function's logic remains the same\n",
    "        history_str = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_history])\n",
    "        try:\n",
    "            result = self.chain.invoke({\"chat_history\": history_str, \"question\": query})\n",
    "            print(f\"--> [Router Agent] Chose path: {result.path_name}\")\n",
    "            return result.path_name\n",
    "        except Exception as e:\n",
    "            print(f\"--> [Router Agent] Error, defaulting to complex query. {e}\")\n",
    "            return \"COMPLEX_QUERY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf15c1eb-591a-4ab2-9002-020191599bfe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Image Fetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47cd9891-9a7a-41a7-9dda-34cdcd6dfbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT USING AS SUCH\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "}\n",
    "\n",
    "def fetch_image_duckduckgo(query):\n",
    "    url = f\"https://duckduckgo.com/?q={query}&t=h_&iar=images&iax=images&ia=images\"\n",
    "    res = requests.get(url, headers=HEADERS)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    images = soup.find_all(\"img\", src=True)\n",
    "    for img in images:\n",
    "        src = img[\"src\"]\n",
    "        if src.startswith(\"http\") and \"data:image\" not in src and not src.endswith(\"favicon.ico\"):\n",
    "            return src\n",
    "    return None\n",
    "\n",
    "def fetch_image_bing(query):\n",
    "    url = f\"https://www.bing.com/images/search?q={query}\"\n",
    "    res = requests.get(url, headers=HEADERS)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    images = soup.find_all(\"img\", src=True)\n",
    "    for img in images:\n",
    "        src = img[\"src\"]\n",
    "        if src.startswith(\"http\") and \"data:image\" not in src and not src.endswith(\"favicon.ico\"):\n",
    "            return src\n",
    "    return None\n",
    "\n",
    "def fetch_image_google(query):\n",
    "    url = f\"https://www.google.com/search?tbm=isch&q={query}\"\n",
    "    res = requests.get(url, headers=HEADERS)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    images = soup.find_all(\"img\", src=True)\n",
    "    for img in images:\n",
    "        src = img[\"src\"]\n",
    "        if src.startswith(\"http\") and \"data:image\" not in src and not src.endswith(\"favicon.ico\"):\n",
    "            return src\n",
    "    return None\n",
    "\n",
    "def save_image(img_url, out_path=\"output_image.jpg\"):\n",
    "    try:\n",
    "        res = requests.get(img_url, headers=HEADERS, stream=True)\n",
    "        if res.status_code == 200:\n",
    "            with open(out_path, \"wb\") as f:\n",
    "                for chunk in res.iter_content(1024):\n",
    "                    f.write(chunk)\n",
    "            return out_path\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download image: {e}\")\n",
    "    return None\n",
    "\n",
    "def fetch_image_anywhere(keyword, out_path=\"output_image.jpg\"):\n",
    "    print(f\"🔍 Searching for: {keyword}\")\n",
    "\n",
    "    # Try DuckDuckGo\n",
    "    print(\"🟡 Trying DuckDuckGo...\")\n",
    "    url = fetch_image_duckduckgo(keyword)\n",
    "    if url:\n",
    "        print(\"✅ Found image from DuckDuckGo\")\n",
    "        return save_image(url, out_path)\n",
    "\n",
    "    # Try Bing\n",
    "    print(\"🟡 Trying Bing...\")\n",
    "    url = fetch_image_bing(keyword)\n",
    "    if url:\n",
    "        print(\"✅ Found image from Bing\")\n",
    "        return save_image(url, out_path)\n",
    "\n",
    "    # Try Google\n",
    "    print(\"🟡 Trying Google...\")\n",
    "    url = fetch_image_google(keyword)\n",
    "    if url:\n",
    "        print(\"✅ Found image from Google\")\n",
    "        return save_image(url, out_path)\n",
    "\n",
    "    print(\"❌ No image found from any source.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def generate_visual_from_answer(answer_text, keyword=None, out_path=\"output_image.jpg\"):\n",
    "    import re\n",
    "\n",
    "    # If keyword is not provided, try extracting from known visualizable topics\n",
    "    if not keyword:\n",
    "        # Define a broad list of common visualizable data science topics\n",
    "        visual_keywords = [\n",
    "            \"confusion matrix\", \"ROC curve\", \"decision tree\", \"random forest\",\n",
    "            \"gradient descent\", \"overfitting\", \"underfitting\", \"k-means clustering\",\n",
    "            \"elbow method\", \"silhouette score\", \"linear regression\", \"logistic regression\",\n",
    "            \"normal distribution\", \"standard deviation\", \"box plot\", \"histogram\",\n",
    "            \"correlation matrix\", \"heatmap\", \"PCA\", \"t-SNE\", \"UMAP\",\n",
    "            \"precision recall\", \"classification report\", \"support vector machine\",\n",
    "            \"bagging\", \"boosting\", \"naive bayes\", \"attention mechanism\",\n",
    "            \"transformer architecture\", \"activation functions\", \"loss functions\",\n",
    "            \"bias variance\", \"feature importance\", \"shap values\", \"lime explanation\"\n",
    "        ]\n",
    "\n",
    "        # Search for the first visual keyword present in the answer\n",
    "        keyword = next((kw for kw in visual_keywords if re.search(rf\"\\b{re.escape(kw)}\\b\", answer_text, re.IGNORECASE)), None)\n",
    "\n",
    "        # If nothing matches, fall back to a default term\n",
    "        if not keyword:\n",
    "            keyword = \"data science diagram\"\n",
    "\n",
    "    print(f\"🔎 Final extracted keyword: {keyword}\")\n",
    "\n",
    "    # Fetch image using fallback logic\n",
    "    image_path = fetch_image_anywhere(keyword, out_path=out_path)\n",
    "\n",
    "    if image_path:\n",
    "        print(f\"📸 Image saved at: {image_path}\")\n",
    "        return image_path\n",
    "    else:\n",
    "        print(\"⚠️ No image fetched.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b07ac0d-0378-4122-9bbe-969f6bf7e83b",
   "metadata": {},
   "source": [
    "## Agent 3 - Synthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8efa7e56-1550-45bd-9594-b9e60c84ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE process_with_rag IN CELL 8 WITH THESE TWO FUNCTIONS\n",
    "# ADD THIS NEW FUNCTION TO CELL 8\n",
    "\n",
    "# In Cell 8, ADD this new function\n",
    "\n",
    "# In Cell 8, REPLACE the run_conversational_response function\n",
    "\n",
    "def run_conversational_response(query: str, llm, user_profile_path: str = None):\n",
    "    \"\"\"\n",
    "    Handles non-technical questions by referencing a persona document.\n",
    "    \"\"\"\n",
    "    print(\"--> [Conversational Path] Generating a persona-driven response.\")\n",
    "\n",
    "    if user_profile_path and os.path.exists(user_profile_path):\n",
    "        print(\"\\n--- Loading the user data:\", user_profile_path)\n",
    "        with open(user_profile_path, 'r') as f:\n",
    "            user_profile_content = f.read()\n",
    "    \n",
    "    AGENT_PERSONA = \"\"\"\n",
    "    Your Identity: You are a helpful and enthusiastic AI Data Science Tutor.\n",
    "    Your Purpose: You were created by a team of developers for the AI Accelerated Spark Challenge, a hackathon hosted by ASU and sponsored by NVIDIA. Your primary mission is to help users learn how to speed up their data science work using NVIDIA GPU acceleration.\n",
    "    Your Capabilities:\n",
    "    - You can answer theoretical questions about data science and GPU computing.\n",
    "    - You can provide code examples for procedural tasks, always prioritizing NVIDIA-based libraries (like CuPy and RAPIDS).\n",
    "    - For procedural questions, you can even run live performance benchmarks on the ASU Sol supercomputer.\n",
    "    - You can personalize your explanations based on the user's knowledge level.\n",
    "    Your Limitations: You cannot answer questions outside of data science and technology. You do not have personal opinions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # THE FIX IS HERE: Added a negative constraint to the prompt.\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are a helpful and enthusiastic AI Tutor. Your identity and capabilities are defined below.\n",
    "        Based on this persona and the user's message, provide a short, friendly, and helpful response.\n",
    "\n",
    "        **IMPORTANT**: For this simple conversational reply, you MUST NOT include any `<think>` or `</think>` tags in your output. Respond directly.\n",
    "\n",
    "        ---\n",
    "        AGENT PERSONA DOCUMENT:\n",
    "        {persona}\n",
    "        ---\n",
    "\n",
    "        **USER'S PROFILE:**\n",
    "        {user_profile}\n",
    "\n",
    "        USER'S MESSAGE:\n",
    "        {question}\n",
    "\n",
    "        YOUR CONVERSATIONAL RESPONSE:\n",
    "        \"\"\",\n",
    "        input_variables=[\"user_profile\", \"persona\", \"question\"]\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"user_profile\": user_profile_content, \"persona\": AGENT_PERSONA, \"question\": query})\n",
    "    print(response)\n",
    "    yield {\"status\": \"llm_complete\", \"content\": response.content}\n",
    "\n",
    "def run_simple_follow_up(query: str, chat_history, llm):\n",
    "    \"\"\"Handles simple follow-up questions without search or benchmarking.\"\"\"\n",
    "    print(\"--> [Simple Path] Handling simple follow-up.\")\n",
    "    history_str = \"\\n\".join([f\"{msg[\"role\"]}: {msg[\"content\"]}\" for msg in chat_history])\n",
    "    \n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "    You are a helpful AI Tutor. Answer the user's follow-up question based on the provided conversation history. Be concise and direct.\n",
    "\n",
    "    CONVERSATION HISTORY:\n",
    "    {chat_history}\n",
    "    \n",
    "    USER'S FOLLOW-UP QUESTION:\n",
    "    {question}\n",
    "\n",
    "    YOUR ANSWER:\n",
    "    \"\"\")\n",
    "    \n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"chat_history\": history_str, \"question\": query})\n",
    "    yield {\"status\": \"llm_complete\", \"content\": response.content}\n",
    "\n",
    "\n",
    "def run_full_pipeline(query: str, chat_history, user_profile_path: str = None):\n",
    "    \"\"\"The new main orchestrator that uses the Router Agent.\"\"\"\n",
    "    \n",
    "    host_node = socket.gethostname()\n",
    "    asurite_id = \"apoojar4\"\n",
    "    llm = ChatOllama(model=\"qwen3:14b\", base_url=f\"http://{asurite_id}@{host_node}:11434/\")\n",
    "    \n",
    "    # 1. First, call the Router Agent to choose a path\n",
    "    router = RouterAgent(llm)\n",
    "    chosen_path = router.choose_path(query, chat_history)\n",
    "    \n",
    "    # 2. Execute the chosen path\n",
    "    if chosen_path == \"CONVERSATIONAL\":\n",
    "        # yield {\"status\": \"llm_complete\", \"content\": \"Hello! How can I help you with data science and GPU acceleration today?\"}\n",
    "        yield from run_conversational_response(query, llm, user_profile_path)\n",
    "    \n",
    "    elif chosen_path == \"FOLLOW_UP\":\n",
    "        # Yield the results from the simple follow-up function\n",
    "        yield from run_simple_follow_up(query, chat_history, llm)\n",
    "        \n",
    "    elif chosen_path == \"COMPLEX_QUERY\":\n",
    "        # Yield the results from our existing full pipeline\n",
    "        # (This is our old process_with_rag function, now just for complex queries)\n",
    "        yield from process_with_rag(query, user_profile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93850901-0361-4fb9-838d-2eed9257268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_rag(query: str, user_profile_path: str = None) -> str:\n",
    "    print(\"\\n--- Running FINAL Integrated RAG Pipeline ---\")\n",
    "    host_node = socket.gethostname()\n",
    "    asurite_id = \"apoojar4\"\n",
    "    llm = ChatOllama(model=\"qwen3:14b\", base_url=f\"http://{asurite_id}@{host_node}:11434/\")\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Load available libraries from file\n",
    "    available_libs = _get_available_libraries(\"installed_libraries.txt\")\n",
    "    library_string = \", \".join(available_libs)\n",
    "\n",
    "    # Call the hybrid retriever to get URLs\n",
    "    # Note: hybrid_retriever is assumed to be defined in the cell above (Cell 5)\n",
    "    urls = hybrid_retriever(query, llm, embedding_model)\n",
    "\n",
    "    context_text = \"\"\n",
    "    if urls:\n",
    "        print(\"-> Found documents. Loading and processing context...\")\n",
    "        docs = [WebBaseLoader(url).load() for url in urls]\n",
    "        docs_list = [item for sublist in docs for item in sublist]\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=500, chunk_overlap=100)\n",
    "        doc_splits = text_splitter.split_documents(docs_list)\n",
    "        vectorstore = Chroma.from_documents(documents=doc_splits, embedding=embedding_model, collection_name=\"rag-chroma\")\n",
    "        retriever = vectorstore.as_retriever()\n",
    "        retrieved_docs = retriever.invoke(query)\n",
    "        context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "        vectorstore.delete_collection()\n",
    "\n",
    "    user_profile_content = \"Knowledge Level: Intermediate\\nTUTORING STRATEGY: The user is at an intermediate level. Provide direct code comparisons (e.g., NumPy vs. CuPy), focus on performance benefits, and show clear benchmarking examples.\"\n",
    "    if user_profile_path and os.path.exists(user_profile_path):\n",
    "        print(\"\\n--- Loading the user data:\", user_profile_path)\n",
    "        with open(user_profile_path, 'r') as f:\n",
    "            user_profile_content = f.read()\n",
    "\n",
    "    # THE DEFINITIVE PROMPT, COMBINING ALL LOGIC: USER PROFILE, 3-PATH REASONING, AND LIBRARY CONSTRAINTS\n",
    "    # In Cell 6, replace the final_prompt_template string with this:\n",
    "\n",
    "    # THE ULTIMATE, BALANCED PROMPT (LOGIC + PERSONA)\n",
    "    final_prompt_template = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are an expert AI Tutor and an enthusiastic advocate for NVIDIA technology. Your primary mission is to provide an encouraging, insightful, and clear answer that is perfectly tailored to the user's knowledge level. Your tone should be that of a patient expert who is excited to help users discover the power of GPU acceleration.\n",
    "\n",
    "        **STEP 1: ANALYZE YOUR INSTRUCTIONS**\n",
    "        - First, read the **TUTORING STRATEGY DOCUMENT**. This is your most important instruction. You MUST tailor the tone, depth, and complexity of your final answer to this level.\n",
    "        - Second, read the user's **QUESTION**. Determine if its intent is **theoretical** (what/why) or **procedural** (how-to).\n",
    "        - Third, note the **CODE GENERATION CONSTRAINT**.\n",
    "\n",
    "        **TUTORING STRATEGY DOCUMENT:**\n",
    "        {user_profile}\n",
    "\n",
    "        **CODE GENERATION CONSTRAINT:**\n",
    "        You MUST only use libraries from the following list: `{available_libraries}`.\n",
    "\n",
    "        **STEP 2: CHOOSE YOUR RESPONSE PATH AND APPLY THE TUTOR PERSONA**\n",
    "        Based on the user's intent, follow ONE of the three paths below.\n",
    "\n",
    "        ---\n",
    "        **PATH 1: The question is THEORETICAL.**\n",
    "        - Provide a clear, insightful, and conversational explanation.\n",
    "        - **TAILOR YOUR TONE:** For a Beginner, use simple analogies and focus on the 'wow factor' of the technology. For an Intermediate user, be technically precise. For an Advanced user, discuss deeper architectural concepts and trade-offs.\n",
    "        - You may include ONE concise code snippet ONLY if it is essential to illustrate a key concept. Do not provide comparative code blocks.\n",
    "\n",
    "        ---\n",
    "        **PATH 2: The question is PROCEDURAL and a common NVIDIA-based GPU library EXISTS for the task.**\n",
    "        - Your answer MUST frame the GPU solution as the modern, high-performance standard.\n",
    "        - **TAILOR YOUR TONE:** For a Beginner, start with an encouraging tone, explaining what the code is doing in simple terms and focusing on how accessible GPU power can be. For an Intermediate user, focus on the performance benefits with clear code comparisons. For an Advanced user, provide more optimized code and add notes about advanced features like CUDA streams or memory management.\n",
    "        - Present the `### Recommended GPU Solution` first, followed by the `### Standard CPU Solution` clearly labeled as a \"baseline for comparison.\"\n",
    "        - The code MUST match the user's request exactly.\n",
    "        - Always add a \"Performance Note\" or \"Key Benefits\" section to explain the trade-offs and advantages.\n",
    "\n",
    "        ---\n",
    "        **PATH 3: The question is PROCEDURAL and a common NVIDIA-based GPU library DOES NOT EXIST for the task.**\n",
    "        - **TAILOR YOUR TONE:** Explain the standard solution at a level appropriate for the user.\n",
    "        - Provide the `### Standard Solution` with a clear code example.\n",
    "        - After the solution, add a `### GPU Acceleration Note`. In this section, explain *why* this specific task is not typically GPU-accelerated. Then, proactively pivot to an encouraging and helpful suggestion. Frame it as an exciting next step, explaining where the user *could* apply GPU acceleration in a related part of their workflow.\n",
    "        ---\n",
    "\n",
    "        **STEP 3: GENERATE THE FINAL, PERSONALIZED ANSWER**\n",
    "        Use the **RELEVANT CONTEXT** below to find supporting facts or code if helpful, but always follow your primary logic and persona from STEP 1 and STEP 2.\n",
    "\n",
    "        **RELEVANT CONTEXT FROM WEB SEARCH:**\n",
    "        {context}\n",
    "\n",
    "        **USER'S QUESTION:**\n",
    "        {question}\n",
    "\n",
    "        YOUR FINAL, TAILORED ANSWER MUST START AFTER THIS:\n",
    "        \"\"\",\n",
    "        input_variables=[\"user_profile\", \"question\", \"context\", \"available_libraries\"],\n",
    "    )\n",
    "    \n",
    "    final_chain = final_prompt_template | llm\n",
    "    llm_response_text = final_chain.invoke({\n",
    "        \"user_profile\": user_profile_content,\n",
    "        \"question\": query,\n",
    "        \"context\": context_text,\n",
    "        \"available_libraries\": library_string\n",
    "    }).content\n",
    "    \n",
    "    print(\"--> [Agent 3] Generated conversational answer.\")\n",
    "    print(llm_response_text)\n",
    "\n",
    "    # yield {\"status\": \"llm_complete\", \"content\": llm_response_text}\n",
    "    # image_path = generate_visual_from_answer(llm_response_text)\n",
    "    image_path = None\n",
    "    if image_path:\n",
    "        yield {\"status\": \"llm_complete\", \"content\": llm_response_text, \"image_path\": image_path}\n",
    "    else:\n",
    "        yield {\"status\": \"llm_complete\", \"content\": llm_response_text}\n",
    "\n",
    "    # Code extraction, upscaling, and benchmarking logic remains the same\n",
    "    extracted_code = _extract_python_code(llm_response_text)\n",
    "    cpu_code, gpu_code = extracted_code[\"cpu_code\"], extracted_code[\"gpu_code\"]\n",
    "\n",
    "    if cpu_code and gpu_code:\n",
    "        print(\"--> [Agent 3] Both CPU and GPU code found. Proceeding to benchmark in parallel.\")\n",
    "        \n",
    "        # upscaled_cpu_code = _upscale_code_agentic(cpu_code, llm, library_string)\n",
    "        # upscaled_gpu_code = _upscale_code_agentic(gpu_code, llm, library_string)\n",
    "        \n",
    "        yield {\"status\": \"benchmark_running\", \"content\": \"\\n\\n---\\n*⏳ Submitting parallel benchmark jobs to Sol...*\"}\n",
    "\n",
    "        try:\n",
    "            benchmarker = SolBenchmarker(user=asurite_id)\n",
    "            # Loop through the results as they come in from the parallel jobs\n",
    "            for result in benchmarker.run_benchmark_parallel(cpu_code, gpu_code):\n",
    "                yield result # Pass the result directly up to the UI handler\n",
    "\n",
    "        except ValueError as e:\n",
    "            yield {\"status\": \"error\", \"message\": f\"Benchmark configuration error: {e}\"}\n",
    "    else:\n",
    "        print(\"--> [Agent 3] Did not find both code types. Skipping benchmark.\")\n",
    "\n",
    "    print(\"--- Pipeline Complete ---\")\n",
    "    # return llm_response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24215d5-6697-4a43-94cb-f2477d06e7b0",
   "metadata": {},
   "source": [
    "# Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b727ad29-8c12-4802-947b-83eddff3741b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [Agent 1] Successfully connected to Ollama LLM.\n",
      "* Running on local URL:  http://127.0.0.1:7870\n",
      "* Running on public URL: https://1082cc3da9ce6d9fac.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://1082cc3da9ce6d9fac.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'metadata': {}, 'content': 'Hi. Who am I?', 'options': []}, {'role': 'assistant', 'metadata': {}, 'content': '🧠 Thinking...', 'options': []}]\n",
      "--> [Router Agent] Chose path: CONVERSATIONAL\n",
      "--> [Conversational Path] Generating a persona-driven response.\n",
      "\n",
      "--- Loading the user data: user_profiles/ashwith.txt\n",
      "content='<think>\\nOkay, the user asked, \"Hi. Who am I?\" and their profile is Ashwith. From the profile, I know that Ashwith has some experience with Python, NumPy, and Pandas but isn\\'t familiar with GPU computing or NVIDIA libraries. The task is to respond as an enthusiastic AI Data Science Tutor.\\n\\nFirst, I need to greet them warmly. Then, I should mention their profile details in a friendly way. Maybe start with \"Hi Ashwith!\" to personalize it. Highlight their existing skills in Python and data manipulation. Then, transition into offering help with GPU acceleration, since that\\'s the tutor\\'s main purpose. Keep the tone positive and encouraging, maybe add an emoji to keep it friendly. Make sure not to use any markdown and keep it concise.\\n</think>\\n\\nHi Ashwith! 😊 From your profile, it looks like you\\'re already familiar with Python and data manipulation libraries like NumPy and Pandas—great start! As your AI Data Science Tutor, I\\'m here to help you level up by introducing you to GPU acceleration with NVIDIA tools like CuPy and RAPIDS. Ready to speed up your data science workflows? Let\\'s dive in! 🚀' additional_kwargs={} response_metadata={'model': 'qwen3:14b', 'created_at': '2025-06-28T20:30:12.504159834Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3817905174, 'load_duration': 21504736, 'prompt_eval_count': 764, 'prompt_eval_duration': 324630466, 'eval_count': 243, 'eval_duration': 3471235764, 'model_name': 'qwen3:14b'} id='run--9e8fbee1-5256-43f8-9a32-7d57170810cc-0' usage_metadata={'input_tokens': 764, 'output_tokens': 243, 'total_tokens': 1007}\n",
      "[{'role': 'user', 'metadata': {}, 'content': 'Hi. Who am I?', 'options': []}, {'role': 'assistant', 'metadata': {}, 'content': \"Hi Ashwith! 😊 From your profile, it looks like you're already familiar with Python and data manipulation libraries like NumPy and Pandas—great start! As your AI Data Science Tutor, I'm here to help you level up by introducing you to GPU acceleration with NVIDIA tools like CuPy and RAPIDS. Ready to speed up your data science workflows? Let's dive in! 🚀\", 'options': []}, {'role': 'user', 'metadata': {}, 'content': 'Hi', 'options': []}, {'role': 'assistant', 'metadata': {}, 'content': '🧠 Thinking...', 'options': []}]\n",
      "--> [Router Agent] Chose path: CONVERSATIONAL\n",
      "--> [Conversational Path] Generating a persona-driven response.\n",
      "\n",
      "--- Loading the user data: user_profiles/ashwith.txt\n",
      "content='<think>\\nOkay, the user sent \"Hi\". I need to respond as an enthusiastic AI Data Science Tutor. Let me check the persona again. The purpose is to help learn GPU acceleration for data science. The response should be friendly and helpful.\\n\\nStart with a greeting. Maybe \"Hello, Ashwith!\" to personalize. Then mention the mission of the AI Accelerated Spark Challenge. Offer assistance with GPU acceleration. Ask how I can help them today. Keep it short and positive. Avoid any markdown. Make sure to use the user\\'s name from the profile. Alright, that should cover it.\\n</think>\\n\\nHello, Ashwith! 👋 It\\'s great to meet you! As your AI Data Science Tutor, I\\'m here to help you unlock the power of NVIDIA GPU acceleration for your data science projects. Whether you\\'re curious about optimizing code, speeding up computations, or exploring libraries like CuPy and RAPIDS, I\\'m here to guide you. How can I assist you today? 😊' additional_kwargs={} response_metadata={'model': 'qwen3:14b', 'created_at': '2025-06-28T20:31:20.02262387Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3235405618, 'load_duration': 20055463, 'prompt_eval_count': 760, 'prompt_eval_duration': 324205765, 'eval_count': 201, 'eval_duration': 2890730239, 'model_name': 'qwen3:14b'} id='run--253e1e50-3221-4c8d-81b8-827d5271ac46-0' usage_metadata={'input_tokens': 760, 'output_tokens': 201, 'total_tokens': 961}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Cell 6: Gradio User Interface\n",
    "#\n",
    "# Description: This cell builds the complete Gradio app, integrating all agents.\n",
    "# It includes controls for creating and selecting user profiles, a chat interface for\n",
    "# posing questions to the tutor, and logic to connect the selected user profile to\n",
    "# the RAG pipeline.\n",
    "#\n",
    "\n",
    "# --- Instantiate Agent 1 for use in the UI ---\n",
    "profiler_agent = UserProfilerAgent_V3()\n",
    "\n",
    "with gr.Blocks(\n",
    "    theme=gr.themes.Soft(),\n",
    "    css=\"\"\"\n",
    "        .gradio-container {background-color: #f5f5f5;}\n",
    "    \"\"\"\n",
    ") as demo:\n",
    "    \n",
    "    # --- State Management for user profiles ---\n",
    "    user_state = gr.State({\n",
    "        \"all_users\": profiler_agent.get_user_list(),\n",
    "        \"current_user\": None\n",
    "    })\n",
    "\n",
    "    gr.Markdown(\"<p style='color:black; font-size:32px; text-align:center; font-weight:bolder'>🤖 AI Accelerated Data Science Tutor</p>\")\n",
    "    \n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            gr.Markdown(\"<p style='color:black; font-size:20px;'>Ask a question about a data science task. For a personalized response, create or select a user profile.</p>\")\n",
    "            gr.Markdown(\"<p style='color:black; font-size:16px;'><strong style='color:black;'>Key Features:</strong><br>• ⚙️ Sophisticated Multi-Agent Architecture<br>• 📊 Live, Parallel Benchmarking on the ASU Sol Supercomputer<br>• 🧑‍🏫 Personalized & Persona-Driven Tutoring</p>\")\n",
    "\n",
    "        with gr.Column(scale=1, min_width=300):\n",
    "            user_dropdown = gr.Dropdown(\n",
    "                label=\"Current User\",\n",
    "                choices=user_state.value[\"all_users\"],\n",
    "                interactive=True,\n",
    "                scale=7  # Make the dropdown take up most of the space\n",
    "            )\n",
    "            with gr.Row():\n",
    "                new_user_btn = gr.Button(\n",
    "                    \"✚ New / Update User\",     # Use an icon as the text\n",
    "                    scale=1, # Make the button take up less space\n",
    "                    min_width=50\n",
    "                )\n",
    "\n",
    "    gr.Markdown(\"---\")\n",
    "\n",
    "    # --- Main Chat UI ---\n",
    "    chatbot = gr.Chatbot(label=\"Conversation\", height=500, type=\"messages\")\n",
    "    visual_output = gr.Image(label=\"Visual Explanation\", visible=False, width=600, height=400)\n",
    "\n",
    "    with gr.Accordion(\"🔎 Show Agent's Thought Process\", open=False):\n",
    "        cot_output = gr.Markdown(\"The agent's reasoning will appear here after it responds.\")\n",
    "    \n",
    "    with gr.Row(equal_height=True):\n",
    "        with gr.Column(scale=4):\n",
    "            msg_textbox = gr.Textbox(\n",
    "                label=\"Your Question\",\n",
    "                placeholder=\"e.g., How do I multiply two 100x100 arrays in Python?\",\n",
    "                lines=2,\n",
    "                container=False\n",
    "            )\n",
    "        with gr.Column(scale=1, min_width=150):\n",
    "            submit_btn = gr.Button(\"Ask Tutor\", variant=\"primary\")\n",
    "            clear_btn = gr.Button(\"✚ Clear Conversation\")\n",
    "\n",
    "    # --- Questionnaire Modal (Hidden by default) ---\n",
    "    with gr.Group(visible=False) as profiler_ui_group:\n",
    "        with gr.Blocks() as profiler_modal:\n",
    "            # NEW LINES\n",
    "            gr.Markdown(\"<h3 style='padding-left: 20px; padding-top: 20px;'>User Profile Questionnaire</h3>\")\n",
    "            gr.Markdown(\"<p style='padding-left: 20px; padding-vertical: 8px;'>Please answer the following questions to help me tailor my explanations to your knowledge level.</p>\")\n",
    "            profiler_name_input = gr.Textbox(label=\"First and Last Name\")\n",
    "            \n",
    "            q_inputs = [gr.Textbox(label=f\"Question {i+1}\", visible=False) for i in range(5)]\n",
    "\n",
    "            submit_profile_btn = gr.Button(\"Submit Profile\")\n",
    "            cancel_profile_btn = gr.Button(\"Cancel\")\n",
    "\n",
    "    # ==================================\n",
    "    # GRADIO HANDLER FUNCTIONS\n",
    "    # ==================================\n",
    "\n",
    "    # REPLACE BOTH HANDLER FUNCTIONS IN YOUR GRADIO UI CELL WITH THESE\n",
    "\n",
    "    # --- NEW: Function 1 - Instant UI Update (using ChatMessage) ---\n",
    "    def on_user_submit(user_message, chat_history):\n",
    "        chat_history = chat_history or []\n",
    "        # Append the user's message as a ChatMessage object\n",
    "        chat_history.append(gr.ChatMessage(role=\"user\", content=user_message))\n",
    "        # Append the \"Thinking...\" placeholder as a ChatMessage object\n",
    "        chat_history.append(gr.ChatMessage(role=\"assistant\", content=\"🧠 Thinking...\"))\n",
    "        # This function returns instantly, guaranteeing the UI updates\n",
    "        return gr.update(value=\"\"), chat_history\n",
    "\n",
    "    # --- NEW: Function 2 - Backend Processing (using ChatMessage for streaming) ---\n",
    "    def get_bot_response(chat_history, selected_user):\n",
    "        print(chat_history)\n",
    "        user_message = chat_history[-2][\"content\"] # Get content from the ChatMessage object\n",
    "\n",
    "        # --- Initial check for selected user ---\n",
    "        user_profile_path = os.path.join(profiler_agent.profiles_dir, \"_\".join(selected_user.lower().split()) + \".txt\") if selected_user else None\n",
    "        if not user_profile_path or not os.path.exists(user_profile_path):\n",
    "             chat_history[-1][\"content\"] = \"⚠️ **Warning:** Could not find the selected user profile.\"\n",
    "             yield chat_history, \"User profile not found.\", gr.update(visible=False)\n",
    "             return\n",
    "\n",
    "        # --- Call the backend generator ---\n",
    "        response_generator = run_full_pipeline(user_message, chat_history, user_profile_path)\n",
    "        \n",
    "        benchmark_data = {\"cpu_time\": None, \"gpu_time\": None}\n",
    "        llm_answer_content = \"\"\n",
    "        benchmark_message_index = -1\n",
    "        thought_process = \"Thinking...\"\n",
    "\n",
    "        for update in response_generator:\n",
    "            status = update.get(\"status\")\n",
    "            content = update.get(\"content\", \"\")\n",
    "            \n",
    "            think_match = re.search(r\"<think>(.*?)</think>\", content, re.DOTALL)\n",
    "            if think_match:\n",
    "                thought_process = think_match.group(1).strip()\n",
    "            \n",
    "            clean_content = re.sub(r\"<think>.*?</think>\", \"\", content, flags=re.DOTALL).strip()\n",
    "\n",
    "            if status == \"llm_complete\":\n",
    "                # Update the \"Thinking...\" bubble with the final text answer\n",
    "                image_path = update.get(\"image_path\")\n",
    "                chat_history[-1][\"content\"] = clean_content\n",
    "                # yield chat_history, thought_process\n",
    "                if image_path:\n",
    "                    yield chat_history, thought_process, gr.update(value=image_path, visible=True)\n",
    "                else:\n",
    "                    yield chat_history, thought_process, gr.update(visible=False)\n",
    "            \n",
    "            elif status == \"benchmark_running\":\n",
    "                # Append a new, titled message bubble for the benchmark status\n",
    "                chat_history.append(gr.ChatMessage(\n",
    "                    role=\"assistant\",\n",
    "                    content=clean_content,\n",
    "                    metadata={\"title\": \"⏳ Benchmark in Progress\"}\n",
    "                ))\n",
    "                benchmark_message_index = len(chat_history) - 1\n",
    "                yield chat_history, thought_process, gr.update(visible=False)\n",
    "            \n",
    "            elif status == \"complete\" or status == \"error\":\n",
    "                if status == \"complete\":\n",
    "                    job_type, time_val = update.get(\"job_type\"), update.get(\"time\")\n",
    "                    if job_type == \"gpu\": benchmark_data[\"gpu_time\"] = time_val\n",
    "                    elif job_type == \"cpu\": benchmark_data[\"cpu_time\"] = time_val\n",
    "                else:\n",
    "                    benchmark_data[\"error\"] = update.get('message')\n",
    "\n",
    "                # Re-create the results table\n",
    "                gpu_time_str = f\"{benchmark_data['gpu_time']:.4f}s\" if benchmark_data[\"gpu_time\"] is not None else \"In Progress...\"\n",
    "                cpu_time_str = f\"{benchmark_data['cpu_time']:.4f}s\" if benchmark_data[\"cpu_time\"] is not None else \"In Progress...\"\n",
    "                \n",
    "                benchmark_md = f\"\"\"\n",
    "| Metric | Result |\n",
    "|---|---|\n",
    "| GPU Time | {gpu_time_str} |\n",
    "| CPU Time | {cpu_time_str} |\n",
    "\"\"\"\n",
    "                if benchmark_data.get(\"cpu_time\") and benchmark_data.get(\"gpu_time\"):\n",
    "                    speedup = benchmark_data[\"cpu_time\"] / benchmark_data[\"gpu_time\"]\n",
    "                    benchmark_md += f\"| **Speedup** | **{speedup:.2f}x faster on GPU!** |\\n\"\n",
    "                \n",
    "                if benchmark_data.get(\"error\"):\n",
    "                    benchmark_md = f\"⚠️ Benchmark Error: {benchmark_data['error']}\"\n",
    "\n",
    "                # Update the benchmark status bubble in-place with the live table\n",
    "                if benchmark_message_index != -1:\n",
    "                    chat_history[benchmark_message_index].content = benchmark_md\n",
    "                    # Optionally, update the title when complete\n",
    "                    chat_history[benchmark_message_index].metadata = {\"title\": \"📊 Live Benchmark Results\"}\n",
    "\n",
    "                yield chat_history, thought_process, gr.update(visible=False)\n",
    "\n",
    "    def clear_chat():\n",
    "        return None, \"\", gr.update(visible=False)\n",
    "\n",
    "    # --- User Profile Logic ---\n",
    "    def start_profiling_flow(current_user):\n",
    "        \"\"\"Called when 'New / Update User' is clicked.\"\"\"\n",
    "        questions = profiler_agent._generate_questions_with_llm()\n",
    "        \n",
    "        updates = [gr.update(visible=True)]\n",
    "        updates.append(gr.update(value=current_user if current_user else \"\", visible=True))\n",
    "        \n",
    "        for i in range(5):\n",
    "            if i < len(questions):\n",
    "                updates.append(gr.update(label=questions[i], value=\"\", visible=True))\n",
    "            else:\n",
    "                updates.append(gr.update(visible=False))\n",
    "        \n",
    "        return *updates, questions\n",
    "\n",
    "    # --- FIX 1: Add 'questions' to the function signature to receive it from the state component. ---\n",
    "    def process_profile_submission(user_name, state, questions, *answers):\n",
    "        \"\"\"Called when 'Submit Profile' is clicked.\"\"\"\n",
    "        \n",
    "        if not user_name:\n",
    "            gr.Warning(\"User name cannot be empty!\")\n",
    "            return state, gr.update(choices=state[\"all_users\"], value=state[\"current_user\"]), gr.update(visible=True)\n",
    "\n",
    "        answers_dict = {q: a for q, a in zip(questions, answers) if q and a}\n",
    "        filepath, level = profiler_agent.generate_and_save_report(user_name, answers_dict)\n",
    "        \n",
    "        gr.Info(f\"Profile for {user_name} has been saved! Detected Knowledge Level: **{level}**\")\n",
    "\n",
    "        updated_users = profiler_agent.get_user_list()\n",
    "        state[\"all_users\"] = updated_users\n",
    "        state[\"current_user\"] = user_name\n",
    "        \n",
    "        return state, gr.update(choices=updated_users, value=user_name), gr.update(visible=False)\n",
    "\n",
    "    def cancel_profiling():\n",
    "        return gr.update(visible=False)\n",
    "\n",
    "    def update_current_user(selected_user, state):\n",
    "        state[\"current_user\"] = selected_user\n",
    "        gr.Info(f\"Switched to user: {selected_user}\")\n",
    "        return state\n",
    "\n",
    "    # ==================================\n",
    "    # WIRING UP THE UI EVENTS\n",
    "    # ==================================\n",
    "    \n",
    "    question_state = gr.State([])\n",
    "\n",
    "    clear_btn.click(clear_chat, outputs=[chatbot, cot_output, visual_output], queue=False)\n",
    "\n",
    "    submit_btn.click(\n",
    "        on_user_submit,\n",
    "        [msg_textbox, chatbot],\n",
    "        [msg_textbox, chatbot]\n",
    "    ).then(\n",
    "        get_bot_response,\n",
    "        [chatbot, user_dropdown],\n",
    "        [chatbot, cot_output, visual_output],\n",
    "        show_progress=\"hidden\"\n",
    "    )\n",
    "\n",
    "    msg_textbox.submit(\n",
    "        on_user_submit,\n",
    "        [msg_textbox, chatbot],\n",
    "        [msg_textbox, chatbot]\n",
    "    ).then(\n",
    "        get_bot_response,\n",
    "        [chatbot, user_dropdown],\n",
    "        [chatbot, cot_output, visual_output],\n",
    "        show_progress=\"hidden\"\n",
    "    )\n",
    "\n",
    "    new_user_btn.click(\n",
    "        start_profiling_flow,\n",
    "        inputs=[user_dropdown],\n",
    "        outputs=[profiler_ui_group, profiler_name_input] + q_inputs + [question_state]\n",
    "    )\n",
    "    \n",
    "    submit_profile_btn.click(\n",
    "        process_profile_submission,\n",
    "        # --- FIX 2: Pass the 'question_state' component as an input. ---\n",
    "        inputs=[profiler_name_input, user_state, question_state, *q_inputs],\n",
    "        outputs=[user_state, user_dropdown, profiler_ui_group]\n",
    "    )\n",
    "\n",
    "    cancel_profile_btn.click(cancel_profiling, outputs=[profiler_ui_group])\n",
    "\n",
    "    user_dropdown.change(\n",
    "        update_current_user,\n",
    "        inputs=[user_dropdown, user_state],\n",
    "        outputs=[user_state]\n",
    "    )\n",
    "\n",
    "# --- Launch the Application ---\n",
    "demo.queue().launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c952e1-e1a5-4ac2-bb51-e9d19f8c03d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"numpy<2\"\n",
    "# !pip install --upgrade numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d8612a-1e4f-4468-b60a-f2e2f310401f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai25.07",
   "language": "python",
   "name": "genai25.07"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
